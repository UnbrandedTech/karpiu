import numpy as np
import pandas as pd
from copy import deepcopy
from typing import Optional, List, Union, Dict, Tuple

from .utils import adstock_process, insert_events


def make_trend(
    n_steps: int,
    rw_loc: float = 0.0,
    rw_scale: float = 0.1,
) -> np.array:
    """Generate time-series trend with different methods.

    Args:
        n_steps: Total steps of series
        rw_loc: Location parameter of random walk generated by `np.random.normal()`
        rw_scale: Scale parameter of random walk generated by `np.random.normal()`

    Returns:
        Simulated trend with length equals `series_len`

    """

    # make trend
    rw = np.random.normal(rw_loc, rw_scale, n_steps)
    trend = np.cumsum(rw)

    return trend


def make_seasonality(
    n_steps: int,
    seasonality: float,
    order: int = 3,
    scale: float = 0.05,
) -> np.array:
    """Generate time-series seasonality with different

    Args:
        n_steps: Total steps of series
        seasonality: For example, a weekly series would use seasonality=52
        order: Fourier series order to generate seasonality.
        scale: Scale parameter of seasonality generation by Normal(0, scale).
            When method = 'discrete', it is directly used.
            when method = 'fourier', it is used to multiply with the sin-cosine wave generated series.

    Returns:
        Simulated seasonal time-series

    Notes
    -----
      1. In case of method = 'discrete', only `seasonality - 1` variables will be generated and the rest is
      directly derived to preserve the property of the sum of seasonality equals 1.
      2. In case of method = 'fourier', see https://otexts.com/fpp2/complexseasonality.html
    """

    if seasonality > 1:
        t = np.arange(0, n_steps)
        out = []
        for i in range(1, order + 1):
            x = 2.0 * i * np.pi * t / seasonality
            out.append(np.sin(x))
            out.append(np.cos(x))
        out = np.column_stack(out)
        b = np.random.normal(0, scale, order * 2)
        seas = np.matmul(out, b)
    else:
        seas = np.zeros(n_steps)

    return seas


def make_features(
    n_obs: int,
    loc: np.array,
    scale: np.array,
    sparsity: float = 0.0,
) -> np.array:
    """Generate features for data simulation

    Args:
        n_obs:
        loc:
        scale:
        sparsity: 0 to 1 to control probability (= 1 - sparsity) at time t of a regressor value > 0

    Returns:
        simulated data 2-D array
    """
    if (len(loc.shape) > 2) or (len(scale.shape) > 2):
        raise Exception("Dimension error for loc or scale.")

    if len(loc) != len(scale):
        raise Exception("loc and scale length not equal.")

    n_feat = len(loc)

    # broadcast
    x = np.random.normal(loc.reshape(1, -1), scale.reshape(1, -1), size=(n_obs, n_feat))

    # control probability of regression kick-in
    if (sparsity > 0.0) and (sparsity < 1.0):
        z = np.random.binomial(1, 1 - sparsity, n_obs * n_feat).reshape(n_obs, -1)
        x = x * z

    return x


def make_regression(
    x: np.array,
    coefs: np.array,
    bias: float = 0.0,
    noise_scale: float = 1.0,
    relevance: float = 1.0,
) -> np.array:
    """Generate regression components for paid marketing

    Args:
        x: regressors / features values; first dimension represents observations
        coefs: Values used as regression coefficients
        noise_scale:  Scale parameter in the white noise generation process
        bias: bias or intercept of the regression component; if None, bias is set to zero.
        relevance: 0 to 1; smaller value indicates smaller total number of useful regressor

    Returns
        x: Regressors simulated array. Should be with shape (series_len, num_of_regressors);
        y: Regression derived by product of X and coefficients plus noise
        coefs: Coefficients modified in the process due to sparsity; if sparsity=0,
        it should be identical to coefs input by user
    """
    if len(x.shape) != 2:
        raise Exception("x must be a 2-dimensions array.")

    n_obs, num_of_regressors = x.shape

    if (relevance > 0.0) and (relevance < 1.0):
        num_of_irr_coefs = int(num_of_regressors * (1 - relevance))
        coefs = np.copy(coefs)
        irr_idx = np.random.choice(num_of_regressors, num_of_irr_coefs, replace=False)
        coefs[irr_idx] = 0.0

    noise = np.random.normal(0, noise_scale, n_obs)

    # make observed response
    if bias:
        y = bias + np.matmul(x, coefs) + noise
    else:
        y = np.matmul(x, coefs) + noise

    return y


def make_adstock_matrix(
    n_steps: int,
    peak_step: np.array,
    left_growth: np.array,
    right_growth: np.array,
) -> np.array:
    """Given the peak, left and right growth, generate adstock matrix"""
    res = list()
    for pks, lg, rg in zip(peak_step, left_growth, right_growth):
        y1 = np.power(1 + lg, np.arange(0, pks))
        y2 = np.power(1 + rg, np.arange(0, n_steps - pks)) * y1[-1]
        y = np.concatenate([y1, y2])
        res.append(y / np.sum(y))
    return np.vstack(res)


def make_mmm_daily_data(
    channels_coef: Union[List[float], np.array],
    channels: List[str],
    features_loc: Union[List[float], np.array],
    features_scale: Union[List[float], np.array],
    scalability: Union[List[float], np.array],
    n_steps: int = 365 * 3,
    adstock_args: Optional[Dict[str, np.array]] = None,
    start_date: str = "2019-01-01",
    country: str = "US",
    with_yearly_seasonality: bool = False,
    with_weekly_seasonality: bool = False,
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, List[str]]:
    """The wrapper function to generate final dataset sample for MMM including the core data frame,
    the scalability and saturation dataframe and the adstock dataframe

    Args:
        coefs (Union[List[float], np.array]): _description_
        channels (List[str]): _description_
        features_loc (Union[List[float], np.array]): _description_
        features_scale (Union[List[float], np.array]): _description_
        scalability (Union[List[float], np.array]): _description_
        n_steps (int, optional): _description_. Defaults to 365*3.
        start_date (str, optional): _description_. Defaults to "2019-01-01".
        adstock_args (Optional[Dict[str, np.array]], optional): _description_. Defaults to None.

    Returns:
        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, List[str]]: _description_
    """
    # these loc and scale parameters are chosen to mimic realistic daily data
    n_max_adstock = 0
    adstock_df = None
    if adstock_args is not None:
        adstock_steps = adstock_args["n_steps"]
        n_max_adstock = adstock_steps - 1

    # add more data length if we have adstock
    n_steps += n_max_adstock

    # create the base df
    # extend with earlier days due to adstock
    start_date_real = pd.to_datetime(start_date) - pd.to_timedelta(
        n_max_adstock, unit="D"
    )
    dt_array = pd.date_range(start=start_date_real, periods=n_steps)
    df = pd.DataFrame(dt_array, columns=["date"])
    if country is not None:
        df, event_cols = insert_events(df, date_col="date", country=country)
    else:
        event_cols = list()

    levs = make_trend(n_steps=n_steps, rw_loc=0.0005, rw_scale=0.00245)

    if with_yearly_seasonality:
        yearly_seas = make_seasonality(
            order=3, scale=0.1, n_steps=n_steps, seasonality=365.25
        )
    else:
        yearly_seas = np.zeros(n_steps)

    if with_weekly_seasonality:
        weekly_seas = make_seasonality(
            order=2, scale=0.33, n_steps=n_steps, seasonality=7
        )
    else:
        weekly_seas = np.zeros(n_steps)

    # spend features
    # (n_steps, n_regressors)
    channel_features = make_features(
        n_obs=n_steps, loc=features_loc, scale=features_scale, sparsity=0.25
    )
    # cut features for positivity
    channel_features = np.ceil(np.clip(channel_features, a_min=0, a_max=np.inf))
    channel_feat_med = np.median(channel_features, axis=0)
    channel_transformed_features = np.log(1 + channel_features / channel_feat_med)
    if isinstance(channels_coef, list):
        channels_coef = np.array(channels_coef)

    if event_cols:
        # (n_steps, n_regressors)
        events_features = df[event_cols].values
        events_coef = np.random.normal(loc=0.0, scale=0.33, size=len(event_cols))
        coefs = np.concatenate([channels_coef, events_coef], axis=-1)
    else:
        coefs = channels_coef

    # adstock treatment and create adstock df
    if adstock_args is not None:
        adstock_steps = adstock_args["n_steps"]
        n_max_adstock = adstock_steps - 1
        adstock_matrix = make_adstock_matrix(**adstock_args)
        channel_transformed_features = adstock_process(
            channel_transformed_features, adstock_matrix=adstock_matrix
        )
        columns = ["d_{}".format(x) for x in range(adstock_steps)]
        adstock_df = pd.DataFrame(adstock_matrix, columns=columns, index=channels)
        adstock_df.index.rename("regressor", inplace=True)

        # remove first n_max_adstock observations
        if event_cols:
            events_features = events_features[n_max_adstock:]

        levs = levs[n_max_adstock:]
        weekly_seas = weekly_seas[n_max_adstock:]
        yearly_seas = yearly_seas[n_max_adstock:]
        channel_features = channel_features[n_max_adstock:]
        dt_array = dt_array[n_max_adstock:]
        df = df.loc[n_max_adstock:, :].reset_index(drop=True)

    # make regression component
    if event_cols:
        reg_covariates = np.concatenate(
            [
                channel_transformed_features,
                events_features,
            ],
            axis=-1,
        )
    else:
        reg_covariates = channel_transformed_features
    channels_df = pd.DataFrame(data=channel_features, columns=channels)
    # bias is chosen to make response in a reasonable range ~ 100 to 5k base range
    # noise is generated in this regression module
    reg_comp = make_regression(reg_covariates, coefs=coefs, bias=6.4, noise_scale=0.05)
    # latent response
    y = levs + weekly_seas + yearly_seas + reg_comp

    # turns to elasticity model with log-log transformation
    df["sales"] = np.ceil(np.exp(y))
    df = pd.concat([df, channels_df], axis=1)

    # re-arrange columns
    df = df[["date", "sales"] + channels + event_cols]

    scalability_df = pd.DataFrame(
        data={
            "regressor": channels,
            "scalability": scalability,
        }
    )

    return df, scalability_df, adstock_df, event_cols
